{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -r './requirements.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('../.env')\n",
    "\n",
    "OPENAI_API_KEY = <OPENAI-KEY>\n",
    "PINECONE_API_KEY = <PINECONE_API>\n",
    "\n",
    "PINECONE_INDEX_NAME = 'sky'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aisensy'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PINECONE_INDEX_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Crawling\n",
    "Load data from websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error sending stats to Plausible: error sending request for url (https://plausible.io/api/event)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Welcome to AWS Documentation\n",
      "Select your cookie preferencesWe use essential cookies and similar tools that are necessary to provide our site and services. We use performance cookies to collect anonymous statistics so we can understand how customers use our site and make improvements. Essential cookies cannot be deactivated, but you can click “Customize cookies” to decline performance cookies.  If you agree, AWS and approved third parties will also use cookies to provide useful site features, remember your preferences, and display relevant content, including relevant advertising. To continue without accepting these cookies, click “Continue without accepting.” To make more detailed choices or learn more, click “Customize cookies.”Accept all cookiesContinue without acceptingCustomize cookiesCustomize cookie preferencesWe use cookies and similar tools (collectively, \"cookies\") for the following purposes.EssentialEssential cookies are necessary to provide our site and services and cannot be deactivated. They are usually set in response to your actions on the site, such as setting your privacy preferences, signing in, or filling in forms. PerformancePerformance cookies provide anonymous statistics about how customers navigate our site so we can improve site experience and performance. Approved third parties may perform analytics on our behalf, but they cannot use the data for their own purposes.Allow performance categoryAllowedFunctionalFunctional cookies help us provide useful site features, remember your preferences, and display relevant content. Approved third parties may set these cookies to provide certain site features. If you do not allow these cookies, then some or all of these services may not function properly.Allow functional categoryAllowedAdvertisingAdvertising cookies may be set through our site by us or our advertising partners and help us deliver relevant marketing content. If you do not allow these cookies, you will experience less relevant advertising.Allow advertising categoryAllowedBlocking some types of cookies may impact your experience of our sites. You may review and change your choices at any time by clicking Cookie preferences in the footer of this site. We and selected third-parties use cookies or similar technologies as specified in the AWS Cookie Notice.CancelSave preferencesUnable to save cookie preferencesWe will only store essential cookies at this time, because we were unable to save your cookie preferences.If you want to change your cookie preferences, try again later using the link in the AWS console footer, or contact support if the problem persists.Dismiss\n",
      "\n",
      "Contact UsEnglishCreate an AWS AccountAWS...DocumentationFeedback Preferences Welcome to AWS DocumentationFind user guides, code samples, SDKs & toolkits, tutorials, API & CLI references, and more.Featured contentAmazon EC2Create and run virtual servers in the cloudAmazon S3Object storage built to retrieve any amount of data from anywhereAmazon DynamoDBManaged NoSQL database serviceAmazon RDSSet up, operate, and scale a relational database in the cloudAWS LambdaRun code without thinking about serversAmazon VPCIsolated cloud resourcesChoosing a generative AI serviceDetermine which AWS generative AI services are the best fit for your organizationChoosing an AWS container serviceEvaluate AWS container services for your modern app developmentAmazon Bedrock or Amazon SageMaker?Determine which service is the best fit for your needsGetting started with AWSLearn the fundamentals and start building on AWS. Find best practices to help you launch your first application and get to know the AWS Management Console.Create an AWS account Set up your AWS accountGetting Started Resource Center AWS Cloud Security Hands-on Tutorials Get started with step-by-step tutorials to launch your first applicationAWS Prescriptive Guidance Resources to help you accelerate cloud adoption and modernizationAWS Architecture Center Learn how to architect more effectively on AWSAWS Solutions Library Find vetted solutions and guidance for business and technical use casesProduct guides & referencesFind user guides, developer guides, API references, and CLI references for\n",
      "                your AWS products.All products (324)Product categoryAll products1234567...33Amazon A2IEasily implement human review of ML predictionsMachine LearningAmazon API GatewayBuild, deploy, and manage APIsNetworking & Content DeliveryServerlessAmazon AppFlowNo-code integration for SaaS apps and AWS servicesAnalyticsAmazon Application Recovery Controller (ARC)Move traffic for application disaster recoveryNetworking & Content DeliveryAmazon AppStream 2.0Stream desktop applications securely to a browserEnd User ComputingAmazon AthenaQuery data in Amazon S3 using SQLAnalyticsAmazon AuroraHigh performance managed relational database engineDatabaseAmazon BedrockAccess best-in-class foundation models to build generative AI applicationsMachine LearningAmazon Bedrock or Amazon SageMaker?Determine which service is the best fit for your needsDecision GuidesAmazon BraketAccelerate quantum computing researchQuantum ComputingDeveloper resourcesFind AWS software development kits (SDKs) & toolkits, code examples, and more.AWS CLIThe AWS Command Line Interface (AWS CLI) is a unified tool to\n",
      "                        manage your AWS productsAWS CLI User GuideInstall the latest version of the AWS CLIConfigure the AWS CLISDKs & toolkitsAWS CDKC++GoJavaJavaScriptKotlin.NETPHPPythonRustAzure DevOpsJetBrainsPowerShellVisual StudioVisual Studio CodeView all SDKs & toolkitsCode Example LibraryFind code examples that show you how to use AWS SDKs with AWSAWS Developer Center Explore tutorials and tools to extend the capabilities of your applicationAWS Developer Tools Blog Find blog posts for developers written by AWS expertsAWS re:Post Find expert-reviewed answers, Knowledge Center articles and videos, and curated knowledge pathsPrivacySite termsCookie preferences\n",
      "      © 2024, Amazon Web Services, Inc. or its affiliates. All rights reserved.On this pageWelcome to AWS DocumentationProduct guides & referencesDeveloper resources\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the AWS documentation page you want to retrieve\n",
    "url = \"https://docs.aws.amazon.com/index.html\"\n",
    "\n",
    "# Use Selenium to load the page and execute JavaScript\n",
    "driver = webdriver.Chrome()  # You need to have the ChromeDriver installed and in your PATH\n",
    "driver.get(url)\n",
    "\n",
    "# Get the page source after JavaScript execution\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Parse the page source with BeautifulSoup\n",
    "soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "# Find and print the textual content\n",
    "textual_content = soup.get_text()\n",
    "print(textual_content)\n",
    "with open(f\"out.txt\", 'w', encoding=\"utf-8\") as fp:\n",
    "    fp.write(textual_content)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up OpenAI Embedding process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt, retry_if_not_exception_type\n",
    "from typing import List\n",
    "from uuid import uuid4\n",
    "import textwrap\n",
    "\n",
    "api_key = 'gpt-3.5-turbo'\n",
    "EMBEDDING_MODEL = 'text-embedding-ada-002'\n",
    "EMBEDDING_CTX_LENGTH = 8191\n",
    "EMBEDDING_ENCODING = 'cl100k_base'\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "# @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6), retry=retry_if_not_exception_type(openai.InvalidRequestError))\n",
    "def get_embedding(text_or_tokens, model=EMBEDDING_MODEL):\n",
    "    return client.embeddings.create(input=text_or_tokens, model=model).data[0].embedding\n",
    "\n",
    "def chunk_text(text: str, max_chunk_size: int, overlap_size: int) -> List[str]:\n",
    "    \"\"\"Helper function to chunk a text into overlapping chunks of specified size.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + max_chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += max_chunk_size - overlap_size\n",
    "    return chunks\n",
    "\n",
    "def transform_record(record: dict) -> List[dict]:\n",
    "    \"\"\"Transform a single record as described in the prompt.\"\"\"\n",
    "    max_chunk_size = 500\n",
    "    overlap_size = 100\n",
    "    chunks = chunk_text(record, max_chunk_size, overlap_size)\n",
    "    transformed_records = []\n",
    "    recordId = str(uuid4())\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk_id = f\"{recordId}-{i+1}\"\n",
    "        response=get_embedding(chunk)\n",
    "        transformed_records.append({\n",
    "            'chunk_id': chunk_id,\n",
    "            'chunk_parent_id': recordId,\n",
    "            'chunk_text': chunk,\n",
    "            'vector' : response\n",
    "            # embeddings.append(response['data'][0]['embedding'])\n",
    "            #'sparse_values': splade(chunk)\n",
    "        })\n",
    "    return transformed_records"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pinecone\n",
    "import time\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "\n",
    "index_name = PINECONE_INDEX_NAME\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "spec = ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in pc.list_indexes():\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,\n",
    "        metric='cosine',\n",
    "        spec=spec\n",
    "    )\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and load data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out.txt', 'r', encoding='ISO-8859-1') as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate embeddings and Pickle the results to save money on OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunked_data = []\n",
    "chunk_array = transform_record(file)\n",
    "for chunk in chunk_array:\n",
    "    chunked_data.append(chunk)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format data to load to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_entries_for_pinecone(entries):\n",
    "    \"\"\"\n",
    "    Prepares an array of entries for upsert to Pinecone.\n",
    "    Each entry should have a 'vector' field containing a list of floats.\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for entry in entries:\n",
    "        vector = entry['vector']\n",
    "        id = entry.get('chunk_id', '')\n",
    "        metadata = entry.get('metadata', {'chunk_id': entry.get('chunk_id', ''),'parent_id': entry.get('chunk_parent_id', ''), 'chunk_text': entry.get('chunk_text', '')})\n",
    "        values = [v for v in vector]\n",
    "        # sparse_values = entry['sparse_values']\n",
    "        #vectors.append({'id': id, 'metadata': metadata, 'values': values, 'sparse_values': sparse_values})\n",
    "        vectors.append({'id': id, 'metadata': metadata, 'values': values})\n",
    "    return {'vectors': vectors, 'namespace': ''}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = prepare_entries_for_pinecone(chunked_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsert vectors (sparse and dense) and metadata to Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.50s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # this is our progress bar\n",
    "\n",
    "batch_size = 32  # process everything in batches of 32\n",
    "for i in tqdm(range(0, len(vectors['vectors']), batch_size)):\n",
    "    ids_batch = [id['id'] for id in vectors['vectors'][i:i+batch_size]]\n",
    "    embeds = [id['values'] for id in vectors['vectors'][i:i+batch_size]]\n",
    "    meta = [id['metadata'] for id in vectors['vectors'][i:i+batch_size]]\n",
    "    # sparse_values = [id['sparse_values'] for id in vectors['vectors'][i:i+batch_size]]\n",
    "    upserts = []\n",
    "    # loop through the data and create dictionaries for uploading documents to pinecone index\n",
    "    # for _id, sparse, dense, meta in zip(ids_batch, sparse_values, embeds, meta):\n",
    "    for _id,dense, meta in zip(ids_batch, embeds, meta):\n",
    "        upserts.append({\n",
    "            'id': _id,\n",
    "            # 'sparse_values': sparse,\n",
    "            'values': dense,\n",
    "            'metadata': meta\n",
    "        })\n",
    "    # upload the documents to the new hybrid index\n",
    "    index.upsert(upserts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query Pinecone and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = 8000\n",
    "\n",
    "def retrieve(query):\n",
    "    res = client.embeddings.create(\n",
    "        input=[query],\n",
    "        model='text-embedding-ada-002'\n",
    "    ).data[0].embedding\n",
    "\n",
    "    # retrieve from Pinecone\n",
    "    xq = res\n",
    "    #sq = splade(query)\n",
    "\n",
    "\n",
    "    # get relevant contexts\n",
    "    #res = index.query(xq, top_k=5, include_metadata=True, sparse_vector=sq)\n",
    "    res = index.query(vector=[xq], top_k=5, include_metadata=True)\n",
    "    contexts = [\n",
    "        x['metadata']['chunk_text'] for x in res['matches']\n",
    "    ]\n",
    "\n",
    "    # build our prompt with the retrieved contexts included\n",
    "    prompt_start = (\n",
    "        \"Answer the question based on the context below. If you cannot answer based on the context or general knowledge about Wells Fargo, truthfully answer that you don't know.\\n\\n\"+\n",
    "        \"Context:\\n\"\n",
    "    )\n",
    "    prompt_end = (\n",
    "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    # append contexts until hitting limit\n",
    "    for i in range(1, len(contexts)):\n",
    "        if len(\"\\n\\n---\\n\\n\".join(contexts[:i])) >= limit:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts[:i-1]) +\n",
    "                prompt_end\n",
    "            )\n",
    "            break\n",
    "        elif i == len(contexts)-1:\n",
    "            prompt = (\n",
    "                prompt_start +\n",
    "                \"\\n\\n---\\n\\n\".join(contexts) +\n",
    "                prompt_end\n",
    "            )\n",
    "    return prompt\n",
    "\n",
    "def complete(prompt):\n",
    "    # query text-davinci-003\n",
    "    res = openai.Completion.create(\n",
    "        engine='gpt-3.5-turbo',\n",
    "        prompt=prompt,\n",
    "        temperature=0,\n",
    "        max_tokens=512,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None\n",
    "    )\n",
    "    return res['choices'][0]['text'].strip()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Langchain Memory for conversation chat style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ABC\\AppData\\Local\\Temp\\ipykernel_30292\\2199372902.py:9: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
      "  conversation_with_summary = ConversationChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAIChat\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(api_key=OPENAI_API_KEY ,model_name=\"gpt-3.5-turbo-instruct\")\n",
    "# llm = OpenAIChat(temperature=0,model_name='gpt-3.5-turbo', api_key= )\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm, \n",
    "    # We set a very low max_token_limit for the purposes of testing.\n",
    "    memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=650)\n",
    ")\n",
    "#conversation_with_summary.predict(input=\"Hi, what's up?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample query to Pinecone and OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AWS stands for Amazon Web Services. It is a cloud computing platform\n",
      "that offers a variety of services such as virtual servers, storage,\n",
      "databases, and more. It also provides resources for developers,\n",
      "tutorials, and code examples to help users launch their applications.\n",
      "Additionally, AWS offers a command line interface and software\n",
      "development kits for various programming languages. It is a popular\n",
      "choice for businesses looking to accelerate cloud adoption and\n",
      "modernization.\n"
     ]
    }
   ],
   "source": [
    "query =\"What is AWS\"\n",
    "# first we retrieve relevant items from Pinecone\n",
    "query_with_contexts = retrieve(query)\n",
    "print(textwrap.fill(str(conversation_with_summary.predict(input=query_with_contexts))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear conversation memory if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_with_summary.memory.clear()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop to ask multiple questions and get answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your input (type 'quit' to exit):  aws\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed input:  AWS\n",
      " AWS stands for Amazon Web Services. It is a cloud computing platform\n",
      "that offers a wide range of services such as virtual servers, object\n",
      "storage, managed databases, and more. It also provides tools and\n",
      "resources for developers to build and deploy applications. Some\n",
      "examples of AWS services include Amazon API Gateway, Amazon S3, and\n",
      "AWS Lambda. If you have a specific question about AWS, I'd be happy to\n",
      "help.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your input (type 'quit' to exit):  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting program...\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # Prompt user for input\n",
    "    user_input = input(\"Enter your input (type 'quit' to exit): \")\n",
    "\n",
    "    # Check if user wants to quit\n",
    "    if user_input.lower() == \"quit\":\n",
    "        print(\"Exiting program...\")\n",
    "        break\n",
    "\n",
    "    # Process user input\n",
    "    processed_input = user_input.upper()  # Convert to all uppercase letters\n",
    "    print(\"Processed input: \", processed_input)\n",
    "\n",
    "    query = user_input\n",
    "\n",
    "    # first we retrieve relevant items from Pinecone\n",
    "    query_with_contexts = retrieve(query)\n",
    "\n",
    "    # then we send the context and the query to OpenAI\n",
    "    print(textwrap.fill(str(conversation_with_summary.predict(input=query_with_contexts))) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
